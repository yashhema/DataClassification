Connector Developer Guide
1. Introduction
This guide provides a comprehensive walkthrough for creating new data source connectors for the Asynchronous Data Classification Platform. Connectors are self-contained Python classes that act as a bridge between the core system and an external data source, such as a database, file share, or cloud storage service.
Adhering to the interfaces and patterns outlined in this document is critical to ensure that your new connector integrates seamlessly with the platform's asynchronous architecture, logging framework, and error handling system.
2. Core Concepts
A connector's primary responsibility is to fulfill three main tasks, which are represented by the methods in its interface:
enumerate_objects: Discover all the individual data objects (e.g., files, database tables) within a data source that need to be tracked.
get_object_details: Retrieve detailed metadata about specific data objects.
get_object_content: Fetch the actual content of a data object so it can be classified.
All connector operations must be asynchronous and non-blocking to work correctly with the platform's asyncio event loop.
3. The Connector Interfaces
The system uses two distinct abstract base classes for connectors, located in core.interfaces.worker_interfaces. You must inherit from the one that best matches your data source type.
3.1. IFileDataSourceConnector
This interface should be used for any source that deals with files and directories (e.g., S3, Azure Blob, FTP).
from typing import List, AsyncIterator
from abc import ABC, abstractmethod
from core.models.models import WorkPacket, DiscoveredObject, ContentComponent

class IFileDataSourceConnector(ABC):
    @abstractmethod
    async def enumerate_objects(self, work_packet: WorkPacket) -> AsyncIterator[List[DiscoveredObject]]:
        pass

    @abstractmethod
    async def get_object_details(self, work_packet: WorkPacket) -> List[ObjectMetadata]:
        pass

    @abstractmethod
    async def get_object_content(self, work_packet: WorkPacket) -> AsyncIterator[ContentComponent]:
        pass


3.2. IDatabaseDataSourceConnector
This interface is for structured data sources like relational databases (e.g., PostgreSQL, MySQL).
from typing import List, AsyncIterator, Dict
from abc import ABC, abstractmethod
from core.models.models import WorkPacket, DiscoveredObject

class IDatabaseDataSourceConnector(ABC):
    @abstractmethod
    async def enumerate_objects(self, work_packet: WorkPacket) -> AsyncIterator[List[DiscoveredObject]]:
        pass

    @abstractmethod
    async def get_object_details(self, work_packet: WorkPacket) -> List[ObjectMetadata]:
        pass

    @abstractmethod
    async def get_object_content(self, work_packet: WorkPacket) -> AsyncIterator[Dict]:
        pass


4. Implementing a New Connector
This section provides a step-by-step guide to building a new connector.
Step 1: Create the Connector Class
Create a new Python file for your connector (e.g., s3_connector.py) and define your class, inheriting from the appropriate interface. The constructor must accept the core dependencies (datasource_id, logger, error_handler, etc.) which will be injected by the system.
# In connectors/my_new_connector.py
from core.interfaces.worker_interfaces import IFileDataSourceConnector
from core.logging.system_logger import SystemLogger
from core.errors import ErrorHandler
# ... other imports

class MyNewConnector(IFileDataSourceConnector):
    def __init__(self, datasource_id: str, logger: SystemLogger, error_handler: ErrorHandler, **kwargs):
        self.datasource_id = datasource_id
        self.logger = logger
        self.error_handler = error_handler
        # Initialize any client libraries or connection objects here


Step 2: Implement enumerate_objects
This method must be an async generator that discovers objects and yields them in batches.
async def enumerate_objects(self, work_packet: WorkPacket) -> AsyncIterator[List[DiscoveredObject]]:
    batch = []
    batch_size = 100 # Configurable

    # Your logic to list objects from the source
    # For example, iterating through a directory or API endpoint
    async for item_from_source in self.api_client.list_all_items():
        # Convert the source item into a DiscoveredObject
        discovered_obj = DiscoveredObject(
            object_id=f"{self.datasource_id}:{item_from_source.path}",
            datasource_id=self.datasource_id,
            object_path=item_from_source.path,
            # ... populate other fields
        )
        batch.append(discovered_obj)

        if len(batch) >= batch_size:
            yield batch
            batch = []

    if batch:
        yield batch


Step 3: Implement get_object_content
This is the most critical method for classification. It must also be an async generator.
For File-Based Connectors:
You must download the file to a temporary local path and pass it to the ContentExtractor. The extractor will then yield ContentComponent objects, which your method must, in turn, yield to the Worker.
async def get_object_content(self, work_packet: WorkPacket) -> AsyncIterator[ContentComponent]:
    for object_id in work_packet.payload.object_ids:
        temp_file_path = None
        try:
            # 1. Download the file asynchronously
            file_path_on_source = self._get_path_from_object_id(object_id)
            temp_file_path = await self._download_file_async(file_path_on_source)

            # 2. Use the ContentExtractor to process the local file
            # The extractor is a dependency of your connector
            async for component in self.content_extractor.extract_from_file(
                temp_file_path, object_id, ...
            ):
                # 3. Yield each component up to the Worker
                yield component

        finally:
            # 4. Clean up the temporary file
            if temp_file_path:
                await aiofiles.os.remove(temp_file_path)


For Database Connectors:
You should query the database for rows and yield them as dictionaries.
async def get_object_content(self, work_packet: WorkPacket) -> AsyncIterator[Dict]:
    for object_id in work_packet.payload.object_ids:
        schema, table = self._parse_object_id(object_id)
        
        # Stream rows from the database asynchronously
        async for row in self.db_client.stream_rows_from_table(schema, table):
            yield {
                "object_id": object_id,
                "content": row # row should be a dictionary
            }


Step 4: Handling Synchronous Libraries
If the client library for your data source is not natively async (like the smbprotocol library), you must not make blocking calls directly. Doing so will freeze the entire application.
Instead, wrap all blocking I/O calls in asyncio.get_event_loop().run_in_executor() to run them in a separate thread pool.
import asyncio

async def make_blocking_call_async(self, some_param):
    loop = asyncio.get_running_loop()

    def sync_function_wrapper():
        # This function contains the blocking call
        return self.sync_library.some_blocking_io_call(some_param)

    # The await here pauses this coroutine, allowing the event loop
    # to run other tasks while the blocking call executes in a thread.
    result = await loop.run_in_executor(None, sync_function_wrapper)
    
    return result


5. Logging and Error Handling
Consistent logging and error handling are enforced system-wide. Your connector must use the provided SystemLogger and ErrorHandler instances.
Logging
The SystemLogger automatically injects ambient context (like trace_id and task_id). Use it for all informational and debug messages.
# Good logging practice
self.logger.info(
    "Starting object enumeration for path",
    path="/my/target/path",
    task_id=work_packet.header.task_id
)


Error Handling
Never let an exception from a client library bubble up directly. Catch the specific exception and wrap it in one of the platform's defined error types from core.errors. This ensures errors are properly classified and handled upstream.
from core.errors import NetworkError, RightsError, ErrorType

try:
    # A call to the data source client library
    self.client.connect(...)
except self.client.AuthenticationError as e:
    # Wrap the specific error in a platform-defined error
    raise RightsError(
        f"Authentication failed for host {self.host}",
        ErrorType.RIGHTS_AUTHENTICATION_FAILED,
        host=self.host
    ) from e
except self.client.ConnectionTimeout as e:
    raise NetworkError(
        f"Connection timed out for host {self.host}",
        ErrorType.NETWORK_CONNECTION_FAILED,
        host=self.host
    ) from e


By following this guide, you can develop robust, high-performance connectors that extend the reach of the Data Classification Platform to any data source.
