# System Deployment Models & Operational Flow

This document details the different ways the system can be deployed and the specific operational flow of data and commands within each model. The architecture is designed to be flexible, supporting everything from a simple, single-process setup for development to a distributed, federated model for enterprise scale.

---

### 1. Single-Process Model

This is the simplest deployment, designed for development, testing, and small-scale use cases. All components run as threads within a single parent process.

* **Architecture**:
    * A single Python process is started via `src/main.py`.
    * The **Orchestrator**, **Resource Coordinator**, and **Webservice API** run as primary objects/threads.
    * A fixed pool of **Worker** threads is created and managed by the Orchestrator.
    * Communication between the Orchestrator and Workers happens via in-memory queues and direct, thread-safe method calls.

* **Operational Flow**:
    1.  The `main.py` script is executed. It loads the system configuration and determines the `deployment_model` is `SINGLE_PROCESS`.
    2.  It initializes the main `Orchestrator` object.
    3.  The Orchestrator starts its internal background threads (Task Assignment, Reaper, etc.).
    4.  It reads the `worker.in_process_thread_count` parameter and starts the configured number of Worker threads.
    5.  It launches the `api.py` webservice (e.g., FastAPI) in a separate thread to listen for external commands.
    6.  A user sends a request to the webservice to start a job (e.g., `POST /start_job`).
    7.  The Orchestrator creates the job and initial tasks in the database.
    8.  The Orchestrator's Task Assignment thread finds a `PENDING` task, consults the in-process Resource Coordinator, and if approved, places the `WorkPacket` into a thread-safe in-memory queue.
    9.  An idle Worker thread, which has been blocked waiting on the queue, instantly receives the `WorkPacket` and begins processing.
    10. The Worker thread calls methods directly on the Orchestrator object to report progress and completion.

---

### 2. Distributed (EKS) Model

This is the standard production deployment model, designed for scalability and high availability within a single datacenter or cloud region.

* **Architecture**:
    * The **Orchestrator** service runs as a set of pods in a Kubernetes (EKS) cluster (typically 2-3 for redundancy).
    * The **Worker** fleet runs as a separate set of pods, which can be auto-scaled based on the depth of the task queue.
    * The **Central Database** is an external, managed service (like Amazon RDS).
    * All communication between the Orchestrator and Workers happens over the network via the Orchestrator's API, which is exposed as a Kubernetes service.

* **Operational Flow**:
    1.  The Orchestrator and Worker container images are deployed to the EKS cluster.
    2.  A user sends a request to the Orchestrator's API service endpoint to start a job.
    3.  The Orchestrator creates the job and initial tasks in the central database.
    4.  An idle Worker pod, which has been long-polling the Orchestrator's `/get_task` endpoint, receives a `WorkPacket`.
    5.  The Worker pod executes the task, connecting directly to the central database to write results (e.g., to the staging table).
    6.  During the task, the Worker makes network calls back to the Orchestrator's API (`/update_task_progress`, `/report_heartbeat`) to report its status.
    7.  The Orchestrator's background "pipeliner" process sees the progress records being created in the database and creates the next phase of tasks.
    8.  Another idle Worker pod picks up one of these new tasks, and the cycle continues.

---

### 3. Federated Model

This is the most advanced deployment, designed for global enterprises with data sources in multiple, geographically separate datacenters. It is a "multi-site" extension of the EKS model.

* **Architecture**:
    * A single **Master Orchestrator** is deployed in a central location.
    * Each remote datacenter has its own complete, independent instance of the **Distributed (EKS) Model**, including a **Child Orchestrator**, a **Worker** fleet, and a connection to the local data sources.
    * The Master Orchestrator communicates with the Child Orchestrators via their APIs.

* **Operational Flow**:
    1.  A user defines a job in the Master Orchestrator that targets data sources in multiple datacenters (e.g., `ds_london_01` and `ds_tokyo_01`).
    2.  The **Master Orchestrator** does **not** create tasks itself. Instead, it acts as a high-level coordinator.
    3.  It analyzes the job and determines that it needs to run work in two separate locations.
    4.  It connects to the API of the **Child Orchestrator** in the London datacenter and calls its `/start_job` endpoint, passing a modified job definition that *only* includes the `ds_london_01` data source.
    5.  It does the same for the Tokyo datacenter's Child Orchestrator.
    6.  From this point on, each Child Orchestrator manages its local job independently, using the standard **Distributed (EKS) Model** flow to process its assigned data sources with its local worker fleet.
    7.  The Master Orchestrator periodically polls the `/get_job_status` endpoint of each Child Orchestrator to monitor the overall progress of the parent job. It aggregates the statuses to provide a single, unified view to the user.
