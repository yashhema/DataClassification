# Content Extraction System - Complete Implementation Specification

## Executive Summary

This specification defines the complete content extraction system for enterprise data classification. The system extracts content from various file types through datasource connectors, processes structured and unstructured data, and streams components for PII classification via the EngineInterface.

## System Architecture Overview

### Core Objective
Extract content from all file types across datasource connectors (SMB, S3, Azure Blob, SQL Server, etc.) and stream components to classification engine while maintaining memory efficiency and error resilience.

### Key Design Principles
1. **Memory-Safe Processing** - Serial component streaming with file size limits
2. **Universal Interface** - All connectors implement identical interface contract
3. **Configuration-Driven** - System and datasource-level extraction control
4. **Error Isolation** - Component-level failure handling with fallback strategies
5. **Classification Integration** - Direct integration with EngineInterface methods

## Updated Connector Interface Contract

### IDataSourceConnector Interface Update

```python
from abc import ABC, abstractmethod
from typing import Iterator, List
from core.models.models import WorkPacket, DiscoveredObject, ObjectMetadata, ContentComponent

class IDataSourceConnector(ABC):
    """Updated interface that all datasource connectors must implement"""
    
    @abstractmethod
    def enumerate_objects(self, work_packet: WorkPacket) -> Iterator[List[DiscoveredObject]]:
        """Fast enumeration of objects with batch processing"""
        pass
    
    @abstractmethod  
    def get_object_details(self, work_packet: WorkPacket) -> List[ObjectMetadata]:
        """Detailed metadata retrieval for discovered objects"""
        pass
    
    @abstractmethod
    def get_object_content(self, work_packet: WorkPacket) -> Iterator[ContentComponent]:
        """
        UPDATED: Extract and stream content components for classification
        
        Input: WorkPacket with ClassificationPayload containing object_ids list
        Output: Iterator of ContentComponent objects (memory-safe streaming)
        
        Processing: For each object_id in work_packet:
        1. Download/access complete file (respecting size limits)  
        2. Extract all components (text, tables, images) serially
        3. Stream components one-at-a-time via iterator
        4. Clean up resources after each file
        """
        pass
```

## ContentComponent Data Model

### Core ContentComponent Structure

```python
@dataclass
class ContentComponent:
    """Universal component model for all extracted content types"""
    
    # Identity and location
    object_id: str              # Original file object ID from WorkPacket
    component_type: str         # Component type classification
    component_id: str           # Unique component identifier within file
    parent_path: str            # Original file path or archive virtual path
    
    # Content data
    content: Union[str, bytes]  # Extracted content (text or binary)
    original_size: int          # Original component size in bytes
    extracted_size: int         # Actual extracted size after limits
    is_truncated: bool          # Whether content was size-limited
    
    # Metadata and context
    schema: Dict[str, Any]      # Component structure information
    metadata: Dict[str, Any]    # Extraction metadata and diagnostics
    extraction_method: str      # Extraction technique used
    is_archive_extraction: bool = False  # Flag for archive members

# Add ContentComponent to core/models/models.py:
from dataclasses import dataclass
from typing import Union, Dict, Any

@dataclass
class ContentComponent:
    object_id: str
    component_type: str  
    component_id: str
    parent_path: str
    content: Union[str, bytes]
    original_size: int
    extracted_size: int
    is_truncated: bool
    schema: Dict[str, Any]
    metadata: Dict[str, Any]
    extraction_method: str
    is_archive_extraction: bool = False
```

### Component Type Taxonomy

```python
COMPONENT_TYPES = {
    # Text content
    "text",                    # Plain text content
    "image_ocr",               # OCR extracted text from images
    
    # Structured data
    "table",                   # Extracted table data
    "table_fallback",          # Failed table extraction → text
    
    # Archive content  
    "archive_member",          # Individual files from archives
    
    # Error cases
    "extraction_error",        # Component extraction failed
    "unsupported_format",      # Format not supported
    "file_too_large",          # File exceeds size limits
    "no_content_extractable"   # No extractable content (OCR disabled)
}
```

## Configuration System

### Two-Level Configuration Architecture

#### 1. ConnectorConfiguration (Database Table)
Stores connector-specific technical configurations:

```python
# Database schema: ConnectorConfigurations table
{
    "ConnectorType": "smb",
    "ConfigName": "default", 
    "Configuration": {
        "supported_formats": ["pdf", "docx", "xlsx", "pptx", "html"],
        "archive_formats": ["zip", "tar", "gz"],
        "library_settings": {
            "pdf_table_priority": ["camelot", "tabula"],
            "ocr_languages": ["en", "es", "fr"],
            "temp_directory_base": "/tmp/extraction"
        }
    }
}
```

#### 2. DataSource Configuration (JSON Field)  
Stores business-level extraction policies per datasource:

```python
# datasource.configuration["content_extraction"]
{
    "limits": {
        "max_file_size_mb": 50,
        "max_component_size_mb": 10, 
        "max_text_chars": 500000,
        "max_document_table_rows": 5000,
        "max_archive_members": 100,
        "max_archive_depth": 3,  # -1 = unlimited
        "sampling_strategy": "head"
    },
    "features": {
        "extract_tables": true,
        "extract_pictures": true,
        "extract_archives": true,
        "ocr_enabled": true,
        "preserve_structure": true,
        "include_metadata": true
    }
}
```

### Configuration Access Pattern

```python
class SMBConnector(IDataSourceConnector):
    def __init__(self, datasource_id: str, logger: SystemLogger, 
                 error_handler: ErrorHandler, db_interface: DatabaseInterface):
        
        # Load datasource configuration
        self.datasource_config = self.db.get_datasource_configuration(datasource_id)
        
        # Load connector-specific configuration
        self.connector_config = self.db.get_connector_configuration("smb", "default")
        
        # Parse content extraction config with defaults
        extraction_dict = self.datasource_config.configuration.get('content_extraction', {})
        self.extraction_config = ContentExtractionConfig.from_dict(extraction_dict)
```

### System-Level Override Pattern

```python
# System OCR flag overrides datasource OCR setting
effective_ocr_enabled = (
    self.system_config.use_ocr and 
    self.extraction_config.features.ocr_enabled
)

if not self.system_config.use_ocr and self.extraction_config.features.ocr_enabled:
    self.logger.warning("OCR disabled by system config, ignoring datasource setting",
                       system_ocr=False, datasource_ocr=True)
```

## File Type Processing Matrix

### Supported File Types and Extraction Libraries

| File Type | Primary Library | Table Extraction | Image Extraction | Notes |
|-----------|----------------|------------------|-------------------|--------|
| PDF | PyMuPDF | Camelot → Tabula fallback | Image extraction + OCR | Complete support |
| Word (.docx) | python-docx | Built-in tables | Embedded images + OCR | Complete support |
| Excel (.xlsx) | pandas + openpyxl | Native tables | Charts/images + OCR | Complete support |
| PowerPoint (.pptx) | python-pptx | Slide tables | Slide images + OCR | Complete support |
| HTML | BeautifulSoup | Table elements | Referenced images + OCR | Complete support |
| Text (.txt) | Built-in | None | None | Text only |
| JSON | json library | Array detection | Base64 images | Structured parsing |
| XML | xml.etree | Element parsing | Embedded content | Structured parsing |
| Email (.eml) | email library | HTML body tables | Attachments + OCR | Complete support |
| Archives (.zip, .tar) | zipfile, tarfile | Member processing | Member processing | Recursive extraction |
| Images (.png, .jpg) | pytesseract + PIL | None | OCR extraction | OCR dependent |

### Unsupported File Types (Return "unsupported" component)
- Legacy Office (.doc, .xls, .ppt)
- Apple formats (.pages, .numbers, .keynote)  
- Outlook (.pst, .msg, .ost)
- OneNote (.one)
- Complex archives (.rar, .7z) - optional
- CAD formats (.dwg, .dxf)

### Required Helper Methods Implementation

```python
def _get_base_name(self, file_path: str) -> str:
    """Get base filename without extension for component ID generation"""
    filename = os.path.basename(file_path)
    return filename.split('.')[0] if '.' in filename else filename

def _create_error_component(self, object_id: str, error_message: str) -> ContentComponent:
    """Create error component when extraction fails"""
    return ContentComponent(
        object_id=object_id,
        component_type="extraction_error",
        component_id=f"{object_id}_error_1",
        parent_path="",
        content=error_message,
        original_size=0,
        extracted_size=len(error_message),
        is_truncated=False,
        schema={},
        metadata={"error": error_message},
        extraction_method="error_handler"
    )

def _create_oversized_component(self, object_id: str, file_info: Dict) -> ContentComponent:
    """Create component for files that exceed size limits"""
    return ContentComponent(
        object_id=object_id,
        component_type="file_too_large", 
        component_id=f"{object_id}_oversized_1",
        parent_path=file_info.get("path", ""),
        content=f"File size {file_info.get('size_mb', 0)}MB exceeds limit {self.extraction_config.limits.max_file_size_mb}MB",
        original_size=file_info.get("size_bytes", 0),
        extracted_size=0,
        is_truncated=False,
        schema={},
        metadata={"file_size_mb": file_info.get("size_mb", 0)},
        extraction_method="size_check"
    )

def _convert_camelot_table(self, camelot_table) -> Dict[str, Any]:
    """Convert Camelot table to structured format"""
    df = camelot_table.df
    headers = df.columns.tolist()
    rows = []
    
    for _, row in df.iterrows():
        row_dict = {headers[i]: str(row.iloc[i]) for i in range(len(headers))}
        rows.append(row_dict)
    
    return {
        "headers": headers,
        "rows": rows,
        "row_count": len(rows),
        "column_count": len(headers)
    }

def _convert_docx_table(self, docx_table) -> Dict[str, Any]:
    """Convert Word document table to structured format"""
    headers = []
    rows = []
    
    # Extract headers from first row
    if docx_table.rows:
        first_row = docx_table.rows[0]
        headers = [cell.text.strip() for cell in first_row.cells]
    
    # Extract data rows
    for row in docx_table.rows[1:]:
        row_data = {}
        for i, cell in enumerate(row.cells):
            header = headers[i] if i < len(headers) else f"column_{i}"
            row_data[header] = cell.text.strip()
        rows.append(row_data)
    
    return {
        "headers": headers,
        "rows": rows,
        "row_count": len(rows),
        "column_count": len(headers)
    }

def _get_size_metadata(self, content: Union[str, bytes]) -> Dict[str, int]:
    """Get size metadata for content"""
    if isinstance(content, str):
        size_bytes = len(content.encode('utf-8'))
    else:
        size_bytes = len(content)
    
    return {
        "original_size": size_bytes,
        "extracted_size": size_bytes
    }
```

### File Processing Flow

```python
def process_file_for_extraction(self, file_object_id: str, work_packet: WorkPacket):
    """Memory-safe file processing pattern"""
    
    temp_file_path = None
    try:
        # 1. Pre-validation (before downloading)
        file_info = self.get_file_metadata(file_object_id)
        if file_info.size_mb > self.extraction_config.limits.max_file_size_mb:
            yield self._create_oversized_component(file_object_id, file_info)
            return
        
        # 2. Download complete file to temp storage
        temp_file_path = self.download_file_to_temp(file_object_id)
        
        # 3. Detect file type and route to appropriate extractor
        file_type = self.detect_file_type(temp_file_path)
        
        # 4. Stream components one-at-a-time (memory safe)
        for component in self.extract_components_serially(temp_file_path, file_type):
            yield component
            # Previous component can be garbage collected
            
    except Exception as e:
        yield self._create_error_component(file_object_id, str(e))
    finally:
        # 5. Always cleanup temp files
        self.cleanup_temp_file(temp_file_path)
```

### Resource Management Integration

```python
# Integration with ResourceCoordinator
def validate_extraction_resources(self, file_size_mb: int) -> bool:
    """Check if file can be processed within resource limits"""
    
    current_memory = self.resource_coordinator.get_current_memory_usage()
    worker_memory_budget = self.system_config.total_process_memory_limit_mb
    
    # Reserve 30% for overhead and libraries
    available_memory = worker_memory_budget * 0.7
    
    if current_memory + file_size_mb > available_memory:
        self.logger.warning("Insufficient memory for file processing",
                           file_size_mb=file_size_mb,
                           current_memory=current_memory,
                           available_memory=available_memory)
        return False
    
    return True
```

## Archive Processing

### Archive as Directory Structure

Archives are treated as expanded directory structures where each member becomes a separate file:

```python
# Archive: contracts.zip
# Members processed as separate files:
contracts.zip/
├── document1.pdf → file_path: "contracts.zip/document1.pdf"
├── spreadsheet.xlsx → file_path: "contracts.zip/spreadsheet.xlsx"  
└── folder/report.docx → file_path: "contracts.zip/folder/report.docx"

# Each member then has internal components:
# document1.pdf components:
#   - component_id: "document1_pdf_text_1"
#   - component_id: "document1_pdf_table_1"
```

### Archive Processing Logic

```python
def process_archive(self, archive_path: str, current_depth: int = 0) -> Iterator[ContentComponent]:
    """Process archive with depth control and circular reference detection"""
    
    # Check depth limits
    if self.extraction_config.limits.max_archive_depth != -1:
        if current_depth > self.extraction_config.limits.max_archive_depth:
            yield self._create_depth_exceeded_component(archive_path)
            return
    
    # Track processed paths for circular reference detection
    if archive_path in self.processed_archive_paths:
        self.logger.warning("Circular archive reference detected, aborting",
                           archive_path=archive_path)
        yield self._create_circular_reference_component(archive_path)
        return
    
    self.processed_archive_paths.add(archive_path)
    
    try:
        # Stream archive members one-at-a-time
        for member_info in self.enumerate_archive_members(archive_path):
            
            # Apply member limits
            if len(self.processed_members) >= self.extraction_config.limits.max_archive_members:
                break
            
            # Extract member to temp location
            member_temp_path = self.extract_archive_member(archive_path, member_info)
            
            try:
                # Process member as regular file
                member_object_id = f"{archive_path}/{member_info.name}"
                
                for component in self.process_file_for_extraction(member_object_id, work_packet):
                    # Mark as archive extraction
                    component.is_archive_extraction = True
                    component.parent_path = f"{archive_path}/{member_info.name}"
                    yield component
                    
            finally:
                self.cleanup_temp_file(member_temp_path)
                
    finally:
        self.processed_archive_paths.remove(archive_path)
```

## Content Extraction Implementation

### PDF Processing

```python
def extract_pdf_components(self, file_path: str, object_id: str) -> Iterator[ContentComponent]:
    """Complete PDF content extraction with fallback strategies"""
    
    try:
        import pymupdf as fitz
        doc = fitz.open(file_path)
        
        try:
            # 1. Extract text content
            text_content = ""
            for page in doc:
                text_content += page.get_text()
            
            if text_content.strip():
                yield ContentComponent(
                    object_id=object_id,
                    component_type="text",
                    component_id=f"{self._get_base_name(file_path)}_pdf_text_1",
                    content=text_content[:self.extraction_config.limits.max_text_chars],
                    is_truncated=len(text_content) > self.extraction_config.limits.max_text_chars,
                    extraction_method="pymupdf_text",
                    parent_path=file_path,
                    **self._get_size_metadata(text_content)
                )
            
            # 2. Extract tables (if enabled)
            if self.extraction_config.features.extract_tables:
                yield from self._extract_pdf_tables(file_path, object_id, doc)
            
            # 3. Extract images (if enabled)
            if self.extraction_config.features.extract_pictures:
                yield from self._extract_pdf_images(file_path, object_id, doc)
                
        finally:
            doc.close()
            
    except Exception as e:
        self.logger.error("PDF processing failed", 
                         file_path=file_path, error=str(e))
        yield self._create_error_component(object_id, f"PDF extraction failed: {str(e)}")

def _extract_pdf_tables(self, file_path: str, object_id: str, doc) -> Iterator[ContentComponent]:
    """PDF table extraction with library fallback"""
    
    table_index = 1
    
    try:
        # Primary: Camelot
        import camelot
        tables = camelot.read_pdf(file_path, pages='all')
        
        for table in tables:
            if table_index > self.extraction_config.limits.max_document_table_rows:
                break
                
            # Convert table to structured format
            table_data = self._convert_camelot_table(table)
            
            yield ContentComponent(
                object_id=object_id,
                component_type="table",
                component_id=f"{self._get_base_name(file_path)}_pdf_table_{table_index}",
                content="",  # Empty - table data in schema
                schema=table_data,
                extraction_method="camelot",
                parent_path=file_path,
                **self._get_size_metadata("")
            )
            table_index += 1
            
    except Exception as camelot_error:
        self.logger.warning("Camelot table extraction failed, trying Tabula",
                           file_path=file_path, error=str(camelot_error))
        
        try:
            # Fallback: Tabula  
            import tabula
            tables = tabula.read_pdf(file_path, pages='all')
            
            for table_df in tables:
                if table_index > self.extraction_config.limits.max_document_table_rows:
                    break
                    
                table_data = self._convert_tabula_table(table_df)
                
                yield ContentComponent(
                    object_id=object_id,
                    component_type="table",
                    component_id=f"{self._get_base_name(file_path)}_pdf_table_{table_index}",
                    content="",  # Empty - table data in schema
                    schema=table_data,
                    extraction_method="tabula_fallback",
                    parent_path=file_path,
                    **self._get_size_metadata("")
                )
                table_index += 1
                
        except Exception as tabula_error:
            self.logger.error("Both table extraction methods failed",
                             file_path=file_path, 
                             camelot_error=str(camelot_error),
                             tabula_error=str(tabula_error))
            
            # Final fallback: Return as text content  
            yield ContentComponent(
                object_id=object_id,
                component_type="table_fallback",
                component_id=f"{self._get_base_name(file_path)}_pdf_table_fallback_1",
                content="Table extraction failed, content processed as text",
                extraction_method="text_fallback",
                parent_path=file_path,
                **self._get_size_metadata("Table extraction failed")
            )
```

### Word Document Processing

```python
def extract_docx_components(self, file_path: str, object_id: str) -> Iterator[ContentComponent]:
    """Complete Word document extraction"""
    
    try:
        from docx import Document
        doc = Document(file_path)
        
        # 1. Extract text content
        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
        text_content = '\n'.join(paragraphs)
        
        if text_content.strip():
            yield ContentComponent(
                object_id=object_id,
                component_type="text",  
                component_id=f"{self._get_base_name(file_path)}_docx_text_1",
                content=text_content[:self.extraction_config.limits.max_text_chars],
                is_truncated=len(text_content) > self.extraction_config.limits.max_text_chars,
                extraction_method="python_docx_paragraphs",
                parent_path=file_path,
                **self._get_size_metadata(text_content)
            )
        
        # 2. Extract tables (if enabled)
        if self.extraction_config.features.extract_tables:
            table_index = 1
            for table in doc.tables:
                table_data = self._convert_docx_table(table)
                
                yield ContentComponent(
                    object_id=object_id,
                    component_type="table",
                    component_id=f"{self._get_base_name(file_path)}_docx_table_{table_index}",
                    content=table_data["serialized_rows"],
                    schema=table_data["schema"],
                    extraction_method="python_docx_tables",
                    parent_path=file_path,
                    **self._get_size_metadata(table_data["serialized_rows"])
                )
                table_index += 1
        
        # 3. Extract embedded images (if enabled)
        if self.extraction_config.features.extract_pictures:
            yield from self._extract_docx_images(file_path, object_id, doc)
            
    except Exception as e:
        self.logger.error("Word document processing failed",
                         file_path=file_path, error=str(e))
        yield self._create_error_component(object_id, f"Word extraction failed: {str(e)}")
```

### Excel Processing

```python
def extract_xlsx_components(self, file_path: str, object_id: str) -> Iterator[ContentComponent]:
    """Complete Excel workbook extraction"""
    
    try:
        import pandas as pd
        
        # Get all sheet names
        excel_file = pd.ExcelFile(file_path)
        
        for sheet_index, sheet_name in enumerate(excel_file.sheet_names, 1):
            try:
                # Read sheet data
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                
                # Convert to table component
                table_data = self._convert_excel_sheet(df, sheet_name)
                
                yield ContentComponent(
                    object_id=object_id,
                    component_type="table",
                    component_id=f"{self._get_base_name(file_path)}_xlsx_sheet_{sheet_index}",
                    content=table_data["serialized_rows"],
                    schema=table_data["schema"],
                    metadata={"sheet_name": sheet_name, "sheet_index": sheet_index},
                    extraction_method="pandas_excel",
                    parent_path=file_path,
                    **self._get_size_metadata(table_data["serialized_rows"])
                )
                
            except Exception as sheet_error:
                self.logger.warning("Failed to process Excel sheet",
                                   sheet_name=sheet_name, error=str(sheet_error))
                continue
        
        # Extract embedded images/charts (if enabled)
        if self.extraction_config.features.extract_pictures:
            yield from self._extract_xlsx_images(file_path, object_id)
            
    except Exception as e:
        self.logger.error("Excel processing failed",
                         file_path=file_path, error=str(e))
        yield self._create_error_component(object_id, f"Excel extraction failed: {str(e)}")
```

## Error Handling and Logging Patterns

### Error Handling Strategy

The document defines two distinct error handling patterns based on error scope:

**Component-Level Errors (Continue Processing):**
```python
# Use when individual component extraction fails but other components can still be processed
try:
    for component in self.extract_table_components(file_path, object_id):
        yield component
except Exception as e:
    # Log the error but don't stop processing
    self.logger.warning("Table extraction failed, attempting text fallback", 
                       file_path=file_path, error=str(e))
    
    # Yield error component and continue to next component type
    yield self._create_error_component(object_id, f"Table extraction failed: {str(e)}")
    # Continue processing images, archives, etc.
```

**System-Level Errors (May Stop Processing):**
```python
# Use when fundamental system issues occur that may affect entire extraction
try:
    connector_config = self.db.get_connector_configuration("smb", "default")
except Exception as e:
    # Use error handler for system-level issues
    error = self.error_handler.handle_error(
        e, "connector_configuration_load",
        operation="system_initialization",
        datasource_id=self.datasource_id
    )
    self.logger.error("Failed to load connector configuration", error_id=error.error_id)
    raise  # Stop processing for system-level failures
```

**Decision Criteria:**
- **Component-level**: Individual file/component processing failures → yield error component, continue
- **System-level**: Configuration, database, or resource failures → use error handler, may stop

### Content Extraction Module Architecture

**Extraction Module Structure:**
```python
# Create new file: src/extraction/content_extractor.py
class ContentExtractor:
    """Reusable content extraction module for all connectors"""
    
    def __init__(self, extraction_config: ContentExtractionConfig, 
                 system_config: SystemConfig, logger: SystemLogger):
        self.extraction_config = extraction_config
        self.system_config = system_config  
        self.logger = logger
        
    def extract_from_file(self, file_path: str, object_id: str) -> Iterator[ContentComponent]:
        """Main extraction method used by all connectors"""
        # Implementation details as shown in document examples
        pass
```

**Connector Integration Pattern:**
```python
class SMBConnector(IDataSourceConnector):
    def __init__(self, datasource_id: str, ...):
        # Initialize connector-specific components
        self.content_extractor = ContentExtractor(
            self.extraction_config,
            self.system_config, 
            self.logger
        )
    
    def get_object_content(self, work_packet: WorkPacket) -> Iterator[ContentComponent]:
        """Use shared extraction module"""
        for object_id in work_packet.payload.object_ids:
            file_path = self.get_file_path_from_object_id(object_id)  # Connector-specific
            
            # Delegate to shared extraction module
            for component in self.content_extractor.extract_from_file(file_path, object_id):
                yield component
```

**Benefits of Shared Extraction Module:**
- **Code reuse**: Same extraction logic across all file-based connectors
- **Consistency**: Identical component structure and error handling
- **Maintenance**: Single place to update extraction algorithms
- **Testing**: Centralized testing of extraction logic

**Connector-Specific Responsibilities:**
- File access and download (SMB vs S3 vs Azure Blob)
- Path resolution and object ID mapping
- Authentication and connection management
- Datasource-specific metadata collection

### Component-Level Error Handling

```python
def extract_components_with_error_isolation(self, file_path: str, object_id: str) -> Iterator[ContentComponent]:
    """Process components with individual error handling"""
    
    file_type = self.detect_file_type(file_path)
    base_name = self._get_base_name(file_path)
    
    # Text extraction (always attempted)
    try:
        for component in self.extract_text_component(file_path, object_id):
            self.logger.info("Text component extracted successfully",
                           component_id=component.component_id,
                           component_type=component.component_type,
                           extraction_method=component.extraction_method)
            yield component
    except Exception as e:
        error = self.error_handler.handle_error(
            e, f"text_extraction_{object_id}",
            operation="text_component_extraction",
            file_path=file_path,
            component_type="text"
        )
        self.logger.error("Text extraction failed", error_id=error.error_id)
        yield self._create_error_component(object_id, f"Text extraction failed: {str(e)}")
    
    # Table extraction (if enabled)
    if self.extraction_config.features.extract_tables:
        try:
            for component in self.extract_table_components(file_path, object_id):
                self.logger.info("Table component extracted successfully",
                               component_id=component.component_id,
                               rows_extracted=component.schema.get("row_count", 0),
                               extraction_method=component.extraction_method)
                yield component
        except Exception as e:
            error = self.error_handler.handle_error(
                e, f"table_extraction_{object_id}",
                operation="table_component_extraction",
                file_path=file_path,
                component_type="table",
                error_category="TABLE_EXTRACTION_FAILURE"
            )
            self.logger.warning("Table extraction failed, attempting text fallback", 
                               error_id=error.error_id)
            
            # Fallback to text extraction
            try:
                fallback_component = self._create_table_fallback_component(object_id, file_path)
                yield fallback_component
            except Exception as fallback_error:
                self.logger.error("Table fallback also failed", 
                                 fallback_error=str(fallback_error))
    
    # Image extraction (if enabled and OCR available)
    if (self.extraction_config.features.extract_pictures and 
        self.system_config.use_ocr and 
        self.extraction_config.features.ocr_enabled):
        
        try:
            for component in self.extract_image_components(file_path, object_id):
                self.logger.info("Image component extracted successfully",
                               component_id=component.component_id,
                               ocr_text_length=len(component.content),
                               extraction_method=component.extraction_method)
                yield component
        except Exception as e:
            error = self.error_handler.handle_error(
                e, f"image_extraction_{object_id}",
                operation="image_component_extraction",
                file_path=file_path,
                component_type="image_ocr",
                error_category="OCR_FAILURE"
            )
            self.logger.warning("Image extraction failed", error_id=error.error_id)
            # Continue processing - image extraction failure is non-blocking
```

### Resource Cleanup Logging

```python
def cleanup_extraction_resources(self, temp_files: List[str], extraction_id: str):
    """Comprehensive resource cleanup with logging"""
    
    cleanup_success = []
    cleanup_failures = []
    
    for temp_file in temp_files:
        try:
            if os.path.exists(temp_file):
                os.remove(temp_file)
                cleanup_success.append(temp_file)
                self.logger.debug("Temp file cleaned up successfully", 
                                 temp_file=temp_file, extraction_id=extraction_id)
            else:
                self.logger.debug("Temp file already removed", 
                                 temp_file=temp_file, extraction_id=extraction_id)
                
        except Exception as cleanup_error:
            cleanup_failures.append({"file": temp_file, "error": str(cleanup_error)})
            self.logger.warning("Temp file cleanup failed",
                               temp_file=temp_file,
                               error=str(cleanup_error),
                               extraction_id=extraction_id)
    
    # Summary logging
    self.logger.info("Resource cleanup completed",
                     extraction_id=extraction_id,
                     files_cleaned=len(cleanup_success),
                     cleanup_failures=len(cleanup_failures))
    
    if cleanup_failures:
        self.logger.warning("Some cleanup operations failed",
                           extraction_id=extraction_id,
                           failed_cleanups=cleanup_failures)
```

### Configuration Validation and Error Handling

```python
def validate_extraction_configuration(self) -> bool:
    """Validate configuration and log issues"""
    
    validation_errors = []
    
    # System vs datasource flag conflicts
    if not self.system_config.use_ocr and self.extraction_config.features.ocr_enabled:
        validation_errors.append({
            "type": "config_conflict",
            "message": "OCR enabled in datasource but disabled at system level",
            "resolution": "Using system setting (OCR disabled)"
        })
    
    # Resource limit validation
    if self.extraction_config.limits.max_file_size_mb > self.system_config.total_process_memory_limit_mb * 0.7:
        validation_errors.append({
            "type": "resource_limit_exceeded",
            "message": f"max_file_size_mb ({self.extraction_config.limits.max_file_size_mb}MB) exceeds safe memory limit",
            "resolution": "File size limit will be clamped to safe value"
        })
    
    # Library availability validation
    missing_libraries = self._check_library_availability()
    if missing_libraries:
        for lib in missing_libraries:
            validation_errors.append({
                "type": "missing_dependency",
                "message": f"Required library '{lib}' not available",
                "resolution": f"Features requiring '{lib}' will be disabled"
            })
    
    # Log validation results
    if validation_errors:
        self.logger.warning("Configuration validation found issues",
                           validation_errors=validation_errors,
                           datasource_id=self.datasource_id)
        
        for error in validation_errors:
            self.logger.warning(f"Config issue: {error['message']} - {error['resolution']}")
    else:
        self.logger.info("Configuration validation passed",
                        datasource_id=self.datasource_id)
    
    return len([e for e in validation_errors if e["type"] != "config_conflict"]) == 0
```

## Classification Integration

### Component Routing to Classification Methods

```python
def route_component_to_classification(self, component: ContentComponent, 
                                    work_packet: WorkPacket,
                                    engine_interface: EngineInterface) -> List[PIIFinding]:
    """Route component to appropriate classification method"""
    
    try:
        if component.component_type == "table":
            # Route to row-by-row classification
            return self._classify_table_component(component, work_packet, engine_interface)
            
        elif component.component_type in ["text", "image_ocr", "table_fallback", "archive_member"]:
            # Route to document content classification
            return self._classify_text_component(component, work_packet, engine_interface)
            
        elif component.component_type in ["extraction_error", "unsupported_format", "file_too_large"]:
            # No classification needed for error components
            self.logger.info("Skipping classification for error component",
                           component_id=component.component_id,
                           component_type=component.component_type)
            return []
            
        else:
            self.logger.warning("Unknown component type for classification",
                               component_type=component.component_type,
                               component_id=component.component_id)
            return []
            
    except Exception as e:
        error = self.error_handler.handle_error(
            e, f"component_classification_{component.component_id}",
            operation="component_classification_routing",
            component_type=component.component_type
        )
        self.logger.error("Component classification failed", error_id=error.error_id)
        return []

def _classify_table_component(self, component: ContentComponent, 
                             work_packet: WorkPacket,
                             engine_interface: EngineInterface) -> List[PIIFinding]:
    """Classify table component using row-by-row processing"""
    
    findings = []
    
    try:
        # Parse table schema and rows
        table_schema = component.schema
        table_rows = self._parse_table_rows(component.content, table_schema)
        
        # Build table metadata for classification
        table_metadata = {
            "table_name": component.component_id,
            "columns": table_schema.get("headers", []),
            "source_file": component.parent_path,
            "component_type": component.component_type
        }
        
        # Classify each row individually
        for row_index, row_data in enumerate(table_rows):
            row_pk = {"row_index": row_index, "component_id": component.component_id}
            
            row_findings = engine_interface.classify_database_row(
                row_data=row_data,
                row_pk=row_pk,
                table_metadata=table_metadata
            )
            
            # Add component context to findings
            for finding in row_findings:
                finding.context_data["component_id"] = component.component_id
                finding.context_data["component_type"] = component.component_type
                finding.context_data["extraction_method"] = component.extraction_method
            
            findings.extend(row_findings)
        
        self.logger.info("Table component classification completed",
                        component_id=component.component_id,
                        rows_processed=len(table_rows),
                        findings_found=len(findings))
        
        return findings
        
    except Exception as e:
        self.logger.error("Table component classification failed",
                         component_id=component.component_id,
                         error=str(e))
        return []

def _classify_text_component(self, component: ContentComponent,
                            work_packet: WorkPacket, 
                            engine_interface: EngineInterface) -> List[PIIFinding]:
    """Classify text component using document content processing"""
    
    try:
        # Build file metadata for classification
        file_metadata = {
            "file_path": component.parent_path,
            "file_name": os.path.basename(component.parent_path),
            "component_id": component.component_id,
            "component_type": component.component_type,
            "extraction_method": component.extraction_method,
            "extraction_source": "archive_member" if component.is_archive_extraction else "file"
        }
        
        # Add archive context if applicable
        if component.is_archive_extraction:
            archive_parent = component.parent_path.split('/')[0]
            file_metadata["archive_parent"] = archive_parent
        
        findings = engine_interface.classify_document_content(
            content=component.content,
            file_metadata=file_metadata
        )
        
        # Add component context to findings
        for finding in findings:
            finding.context_data["component_id"] = component.component_id
            finding.context_data["component_type"] = component.component_type
            finding.context_data["extraction_method"] = component.extraction_method
        
        self.logger.info("Text component classification completed",
                        component_id=component.component_id,
                        content_length=len(component.content),
                        findings_found=len(findings))
        
        return findings
        
    except Exception as e:
        self.logger.error("Text component classification failed",
                         component_id=component.component_id,
                         error=str(e))
        return []
```

### Classification Context Data Structure

Components are mapped to classification context as follows:

```python
# Regular file component context
classification_context = {
    "file_path": "/data/reports/financial.pdf",
    "file_name": "financial.pdf",
    "field_name": "financial_pdf_table_1",    # component_id used as field
    "component_type": "table",
    "extraction_source": "file",
    "extraction_method": "camelot"
}

# Archive member component context  
classification_context = {
    "file_path": "contracts.zip/document1.pdf",  # Virtual file path
    "file_name": "document1.pdf", 
    "field_name": "document1_pdf_text_1",       # component_id used as field
    "component_type": "text",
    "extraction_source": "archive_member",
    "archive_parent": "contracts.zip",
    "extraction_method": "pymupdf_text"
}
```

## Implementation Guidelines

### Connector Implementation Pattern

```python
class SMBConnector(IDataSourceConnector):
    """Complete SMB connector implementation following specification"""
    
    def __init__(self, datasource_id: str, logger: SystemLogger,
                 error_handler: ErrorHandler, db_interface: DatabaseInterface):
        
        # Standard initialization pattern
        self.datasource_id = datasource_id
        self.logger = logger  
        self.error_handler = error_handler
        self.db = db_interface
        
        # Load configurations
        self.datasource_config = self.db.get_datasource_configuration(datasource_id)
        self.connector_config = self.db.get_connector_configuration("smb", "default")
        
        # Parse content extraction config
        extraction_dict = self.datasource_config.configuration.get('content_extraction', {})
        self.extraction_config = ContentExtractionConfig.from_dict(extraction_dict)
        
        # Initialize extraction components
        self.extractor_registry = self._build_extractor_registry()
        self.processed_archive_paths = set()
        
        # Validate configuration
        self.validate_extraction_configuration()
    
    def get_object_content(self, work_packet: WorkPacket) -> Iterator[ContentComponent]:
        """Main content extraction implementation"""
        
        trace_id = work_packet.header.trace_id
        task_id = work_packet.header.task_id
        payload = work_packet.payload
        
        self.logger.log_database_operation(
            "GET_CONTENT", "SMB", "STARTED",
            trace_id=trace_id,
            task_id=task_id,
            object_count=len(payload.object_ids)
        )
        
        try:
            objects_processed = 0
            
            # Process each object serially (memory safe)
            for object_id in payload.object_ids:
                try:
                    # Stream all components from this object
                    for component in self.process_file_for_extraction(object_id, work_packet):
                        yield component
                        
                    objects_processed += 1
                    
                except Exception as e:
                    # Log object-level error but continue processing
                    error = self.error_handler.handle_error(
                        e, f"extract_content_{object_id}",
                        operation="file_content_extraction",
                        object_id=object_id,
                        trace_id=trace_id
                    )
                    self.logger.warning(f"Failed to extract content from {object_id}",
                                       error_id=error.error_id)
                    
                    # Yield error component and continue
                    yield self._create_error_component(object_id, str(e))
                    continue
            
            # Log completion
            self.logger.log_database_operation(
                "GET_CONTENT", "SMB", "COMPLETED", 
                trace_id=trace_id,
                task_id=task_id,
                objects_processed=objects_processed
            )
            
        except Exception as e:
            error = self.error_handler.handle_error(
                e, "smb_get_object_content",
                operation="content_extraction_batch",
                datasource_id=self.datasource_id,
                trace_id=trace_id,
                task_id=task_id
            )
            self.logger.error("SMB content extraction failed", error_id=error.error_id)
            raise
```

### Library Dependency Management

```python
def _check_library_availability(self) -> List[str]:
    """Check availability of optional extraction libraries"""
    
    missing_libraries = []
    
    # PDF processing libraries
    try:
        import pymupdf
    except ImportError:
        missing_libraries.append("pymupdf")
    
    try:
        import camelot
    except ImportError:
        missing_libraries.append("camelot")
    
    try: 
        import tabula
    except ImportError:
        missing_libraries.append("tabula")
    
    # Office processing libraries
    try:
        from docx import Document
    except ImportError:
        missing_libraries.append("python-docx")
    
    try:
        import openpyxl
    except ImportError:
        missing_libraries.append("openpyxl")
    
    try:
        from pptx import Presentation
    except ImportError:
        missing_libraries.append("python-pptx")
    
    # OCR libraries
    try:
        import pytesseract
        from PIL import Image
    except ImportError:
        missing_libraries.append("pytesseract+PIL")
    
    return missing_libraries

def _build_extractor_registry(self) -> Dict[str, callable]:
    """Build registry of available extractors based on library availability"""
    
    registry = {}
    
    # Always available
    registry["txt"] = self.extract_text_components
    registry["json"] = self.extract_json_components
    registry["xml"] = self.extract_xml_components
    
    # Conditionally available based on library checks
    try:
        import pymupdf
        registry["pdf"] = self.extract_pdf_components
    except ImportError:
        self.logger.warning("PyMuPDF not available, PDF extraction disabled")
    
    try:
        from docx import Document
        registry["docx"] = self.extract_docx_components
    except ImportError:
        self.logger.warning("python-docx not available, Word extraction disabled")
    
    try:
        import pandas as pd
        import openpyxl
        registry["xlsx"] = self.extract_xlsx_components
    except ImportError:
        self.logger.warning("pandas/openpyxl not available, Excel extraction disabled")
    
    try:
        from pptx import Presentation  
        registry["pptx"] = self.extract_pptx_components
    except ImportError:
        self.logger.warning("python-pptx not available, PowerPoint extraction disabled")
    
    try:
        from bs4 import BeautifulSoup
        registry["html"] = self.extract_html_components
    except ImportError:
        self.logger.warning("BeautifulSoup not available, HTML extraction disabled")
    
    return registry
```

## Testing and Validation

### Component Testing Pattern

```python
def test_component_extraction():
    """Test component extraction for all supported file types"""
    
    test_files = {
        "sample.pdf": ["text", "table", "image_ocr"],
        "sample.docx": ["text", "table"],
        "sample.xlsx": ["table"],
        "sample.pptx": ["text", "table", "image_ocr"],
        "sample.html": ["text", "table"],
        "sample.zip": ["archive_member"] 
    }
    
    for filename, expected_components in test_files.items():
        components = list(connector.extract_components_from_file(filename))
        
        # Validate expected component types are present
        found_types = {c.component_type for c in components}
        expected_types = set(expected_components)
        
        assert expected_types.issubset(found_types), f"Missing component types for {filename}"
        
        # Validate component structure
        for component in components:
            assert component.component_id.startswith(filename.split('.')[0])
            assert component.parent_path == filename
            assert len(component.content) > 0
            assert component.extraction_method is not None
```

### Error Handling Validation

```python
def test_error_handling():
    """Test error handling and fallback strategies"""
    
    # Test oversized file handling
    large_component = connector.process_oversized_file("huge_file.pdf", 500)  # 500MB
    assert large_component.component_type == "file_too_large"
    
    # Test corrupted file handling  
    error_component = connector.process_corrupted_file("corrupted.pdf")
    assert error_component.component_type == "extraction_error"
    
    # Test table extraction fallback
    components = list(connector.extract_components_from_file("complex_table.pdf"))
    table_components = [c for c in components if c.component_type in ["table", "table_fallback"]]
    assert len(table_components) > 0  # Should have either table or fallback
    
    # Test OCR disabled scenario
    with system_config.use_ocr = False:
        components = list(connector.extract_components_from_file("image.png"))
        assert components[0].component_type == "no_content_extractable"
```

## Performance Considerations

### Memory Optimization

- **Serial Processing:** Only one file in memory at a time
- **Component Streaming:** Iterator pattern prevents accumulation  
- **Size Limits:** `max_file_size_mb` prevents memory exhaustion
- **Resource Cleanup:** Immediate temp file deletion
- **Library Efficiency:** Use memory-efficient libraries where possible

### Concurrency Model

- **Single-threaded extraction:** Within each worker thread
- **Multiple workers:** Orchestrator manages worker pool
- **Resource coordination:** Shared via ResourceCoordinator
- **Database connections:** Pooled per connector instance

### Scalability Factors

- **Connector instances:** One per datasource per worker
- **Configuration caching:** Database-level caching of configs
- **Temp storage:** Configurable temp directory per system
- **Archive processing:** Depth limits prevent runaway extraction

## Security Considerations

### Archive Security

- **Path traversal protection:** Validate archive member paths
- **Circular reference detection:** Track processed archive paths  
- **Size limits:** Apply member count and depth limits
- **Resource exhaustion:** Honor extraction timeouts and memory limits

### Content Validation

- **File type validation:** Magic bytes verification before processing
- **Size validation:** Pre-download size checks
- **Content sanitization:** OCR text cleanup and validation
- **Error containment:** Component-level error isolation

### Credential Management

- **Configuration separation:** Credentials separate from extraction config
- **Secure storage:** Leverage existing credential management system
- **Access logging:** Log all file access operations with context

## Conclusion

This specification provides complete guidance for implementing content extraction across all datasource connectors. The architecture ensures memory safety, error resilience, configuration flexibility, and seamless integration with the classification engine while maintaining consistent interfaces across all connector types.

### Key Implementation Success Factors

1. **Follow the exact interface contracts** - All connectors must implement identical method signatures
2. **Implement proper error isolation** - Component failures should not break entire extraction
3. **Respect resource limits** - Honor all configuration limits and system constraints  
4. **Maintain logging consistency** - Use established logging patterns with proper context
5. **Test extensively** - Validate extraction, fallback, and error scenarios for all file types

This specification serves as the definitive guide for building robust, scalable, and maintainable content extraction capabilities across the entire enterprise data classification platform.