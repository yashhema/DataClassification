System Design: Asynchronous Data Classification Platform
1. Introduction
This document outlines the architecture of the Asynchronous Data Classification Platform, a high-performance system designed to discover, classify, and report on sensitive data across various enterprise data sources.
The system is built on a modern, asynchronous architecture using Python's asyncio library. This design choice ensures high efficiency and scalability, particularly for I/O-bound operations like network requests and database interactions, allowing the platform to manage thousands of concurrent tasks with minimal resource overhead.
The architecture is decoupled, separating concerns into distinct components that communicate through well-defined interfaces and a central database. This modularity allows for independent development, testing, and scaling of different parts of the system.
2. Core Components
The platform consists of several key components that work in concert to execute classification jobs.
2.1. Orchestrator
The Orchestrator is the central brain of the system. It acts as the primary control plane and is responsible for managing the entire lifecycle of a classification job. Its key responsibilities include:
Job Management: Creating new jobs from templates, tracking their overall status (e.g., RUNNING, PAUSED, COMPLETED), and handling user commands like pausing or canceling a job.
Task Generation (Pipelining): Breaking down a high-level job into a series of smaller, executable tasks. For example, a single discovery job is broken down into an "Enumerate Objects" task, followed by a "Get Details" task, and finally a "Classify Content" task.
Task Assignment: Matching pending tasks with available workers and resources, ensuring that system limits (like the maximum number of concurrent connections to a database) are respected.
System Monitoring: Monitoring the health of jobs and workers. It includes a "Reaper" component that finds and re-queues tasks that have timed out due to worker failure, ensuring system resilience.
2.2. Worker
The Worker is the primary execution engine of the system. Its sole purpose is to receive tasks from the Orchestrator, process them, and report the results. Key characteristics include:
Task Execution: A Worker is capable of executing any type of task defined in the system (e.g., DISCOVERY_ENUMERATE, CLASSIFICATION).
Connector Integration: To perform its work, the Worker uses a Connector specific to the target data source (e.g., an SMB connector for a file share, a SQL Server connector for a database).
Statelessness: Workers are designed to be stateless. All the information they need to perform a task is contained within the "Work Packet" they receive from the Orchestrator. This allows for flexible scaling, as new workers can be added or removed without impacting in-progress jobs.
2.3. Connectors
Connectors are plugins that contain all the logic necessary to interact with a specific type of data source. They act as a bridge between the generic Worker and the target system. The system defines two primary types of connectors:
Database Connectors: Used for structured data sources like SQL Server. They handle tasks like enumerating tables and columns and fetching rows for classification.
File-Based Connectors: Used for unstructured or semi-structured data sources like SMB file shares. They are responsible for enumerating files and directories. A key feature is their integration with the Content Extractor.
2.4. Content Extractor
The Content Extractor is a specialized component used exclusively by file-based connectors. It can parse a wide variety of file types (PDFs, Office documents, archives, etc.), breaking them down into fundamental ContentComponent objects (e.g., paragraphs of text, table rows, OCR text from images). This allows the classification engine to analyze the contents of complex files in a standardized way.
2.5. Database Interface
The Database is the central hub for all persistent state in the system. It is not just a data store but also acts as the primary communication medium between the Orchestrator and the Workers, enabling their decoupled nature. It stores:
System Configuration: All system settings, connector configurations, and classifier definitions.
Job and Task State: The status of all jobs and the queue of pending, assigned, and completed tasks.
Discovered Objects & Findings: The results of the classification process, including the catalog of all discovered data objects and any sensitive findings within them.
3. System Workflow: The Lifecycle of a Task
The interaction between these components follows a well-defined, asynchronous workflow.
Job Creation: A user or a schedule triggers a new job. The Orchestrator creates a job record in the database and generates the initial set of tasks (e.g., a DISCOVERY_ENUMERATE task for each target data source).
Task Assignment: The Orchestrator's Task Assigner component identifies a pending task. It reserves the necessary resources (e.g., a connection slot for the target data source) and waits for a Worker to become available.
Task Acquisition: A Worker polls the Orchestrator for a new task. The Orchestrator assigns the task to the Worker and provides it with a "Work Packet" containing all necessary instructions.
Task Execution: The Worker uses the appropriate Connector to perform the task.
If it's a file-based task, the Connector downloads the file and uses the Content Extractor to parse its contents.
The Connector streams the data (file components or database rows) to the classification engine, which identifies sensitive information.
Result Reporting: The Worker reports its findings back to the central Database. For long-running jobs, it sends progress updates and heartbeats to the Orchestrator to extend its task lease.
Pipelining: The Orchestrator's Pipeliner component sees the output from the completed task (e.g., a list of discovered files) and automatically creates the next task in the sequence (e.g., a CLASSIFICATION task for those files).
Job Completion: This cycle repeats until all tasks for a job are complete. The Orchestrator's Job Monitor then transitions the job's final status to COMPLETED or FAILED.
This asynchronous, database-centric design allows the system to be highly scalable and resilient. New workers can be added to increase processing power, and the Orchestrator can manage failures gracefully, ensuring that no tasks are lost.
