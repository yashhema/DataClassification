Orchestrator-Worker Interface SpecificationThis document defines the formal communication contract between the Orchestrator and Worker components. The interface is designed as a service exposed by the Orchestrator, which Workers call to receive tasks and report progress. This contract is abstract and can be implemented over a network (e.g., REST API, gRPC) or via in-process function calls without changing the core logic.1. Core Concepts & PrinciplesWorker-Initiated Communication: The Worker is always the client. It initiates all communication by calling methods on the Orchestrator service. The Orchestrator never pushes commands to a running Worker.Stateless Workers, Stateful Orchestrator: Workers are stateless and receive all necessary instructions in a WorkPacket. The Orchestrator maintains the authoritative state of all jobs and tasks in the central database.Direct Database Writes: For high-volume data (discovered objects, classification findings), the Worker writes results directly to the database (staging or main tables). It does not send this data back to the Orchestrator through the API.Pipelining via Progress Reporting: To enable parallel processing, a Worker reports its progress in batches to the Orchestrator using a lightweight record. This signals the Orchestrator to create the next phase of tasks before the current task is fully complete.2. Orchestrator API MethodsThese are the service endpoints the Orchestrator exposes.2.1 get_task(worker_id: str) -> WorkPacketPurpose: The primary method for a Worker to request a new unit of work. This is a blocking, long-poll request.Input (worker_id): The unique identifier of the worker process making the request.Output (WorkPacket): A self-contained JSON object with all instructions for the task. Returns null or an empty object if the long-poll times out.Logging/Tracing Context: A new trace_id should be initiated or propagated for this entire task lifecycle. The Orchestrator logs the assignment of task_id to worker_id.2.2 update_task_progress(task_id: int, progress_record: TaskOutputRecord) -> AcknowledgmentPurpose: Enables pipelining. A Worker calls this method during a long-running task to signal that a batch of results has been produced and is ready for the next phase.Input (task_id, progress_record): The ID of the current task and a TaskOutputRecord payload.Output (Acknowledgment): A simple success/failure response.Logging/Tracing Context: The task_id and a unique batch_id from the progress_record are logged.2.3 complete_task(task_id: int, final_summary: dict) -> AcknowledgmentPurpose: Called once by the Worker when a task is fully completed, either successfully or with a failure.Input (task_id, final_summary): The ID of the completed task and a final summary object.Output (Acknowledgment): A simple success/failure response.Logging/Tracing Context: The task_id and the final status (COMPLETED or FAILED) are logged, along with any error details.2.4 report_heartbeat(task_id: int) -> AcknowledgmentPurpose: A simple "I'm still alive" message sent periodically by a Worker during a long task to prevent its lease from expiring.Input (task_id): The ID of the task the Worker is currently processing.Output (Acknowledgment): A simple success/failure response.Logging/Tracing Context: The task_id and worker_id are logged at a DEBUG level to avoid excessive noise.3. Task-Specific Workflows & Data PacketsThis section details the end-to-end process for each major task type.3.1 Task Type: DISCOVERY_ENUMERATEGoal: To perform a fast, recursive scan of one or more locations and list their contents.WorkPacket Structure:{
    "header": {
        "task_id": 101,
        "job_id": 12,
        "trace_id": "abc-123"
    },
    "payload": {
        "task_type": "DISCOVERY_ENUMERATE",
        "datasource_id": "ds_001",
        "paths": ["/share/finance/", "/share/marketing/campaigns/"],
        "staging_table_name": "Staging_DiscoveredObjects_Job_12",
        "config": {
            "batch_write_size": 1000,
            "max_recursion_depth": 5
        }
    }
}
Worker's Internal Workflow:The Worker receives the WorkPacket.It instantiates the appropriate Connector for datasource_id.It calls the Connector's enumerate_objects method, passing the paths list and the config object.The Connector iterates through the paths and begins listing objects.Intermediate Results: For every batch_write_size objects found, the Connector writes the DiscoveredObject records directly to the staging_table_name.After each successful batch write, the Worker calls the Orchestrator's update_task_progress method with a TaskOutputRecord payload:{
    "OutputType": "DISCOVERED_OBJECTS",
    "OutputPayload": {
        "staging_table": "Staging_DiscoveredObjects_Job_12",
        "batch_id": "uuid-for-batch-1",
        "count": 1000
    }
}
This loop continues until the Connector has fully enumerated all given paths.Final Summary: Once finished, the Worker calls the Orchestrator's complete_task method. If the Connector found sub-directories, it includes them in the summary:{
    "status": "COMPLETED",
    "total_objects_found": 15203,
    "new_sub_tasks": [
        { "paths": ["/share/finance/2024/", "/share/finance/archive/"] }
    ]
}
3.2 Task Type: DISCOVERY_GET_DETAILSGoal: To fetch rich, detailed metadata for a batch of discovered objects.WorkPacket Structure:{
    "header": {
        "task_id": 102,
        "job_id": 12,
        "trace_id": "abc-123"
    },
    "payload": {
        "task_type": "DISCOVERY_GET_DETAILS",
        "datasource_id": "ds_001",
        "object_ids": ["hash_of_obj1", "hash_of_obj2", ...],
        "config": {
            "fetch_permissions": true,
            "fetch_table_stats": false
        }
    }
}
Worker's Internal Workflow:The Worker receives the WorkPacket.It instantiates the Connector for datasource_id.It calls the Connector's get_object_details method, passing the object_ids list and config.The Connector fetches the detailed metadata for each object.Data Writing: The Connector writes the ObjectMetadata records directly to the main ObjectMetadata table. This can be done in batches for efficiency.Final Summary: After all objects in the batch are processed, the Worker calls complete_task with a summary:{
    "status": "COMPLETED",
    "success_count": 98,
    "failed_count": 2
}
3.3 Task Type: CLASSIFICATIONGoal: To read the content of a batch of objects and scan them for sensitive data.WorkPacket Structure:{
    "header": {
        "task_id": 205,
        "job_id": 12,
        "trace_id": "abc-123"
    },
    "payload": {
        "task_type": "CLASSIFICATION",
        "datasource_id": "ds_001",
        "classifier_template_id": "template_financial_pii",
        "object_ids": [ "hash_of_obj1", "hash_of_obj2", ... ],
        "config": {
            "max_content_size_mb": 100,
            "sampling_config": {
                "default_sample_size": 1000
            }
        }
    }
}
Worker's Internal Workflow:The Worker receives the WorkPacket.It instantiates the Connector and the Classification Engine.It iterates through the object_ids. For each object:a. It calls the Connector's get_object_content method, respecting the config parameters (e.g., content size limits, table sampling).b. It passes the content to the Classification Engine's classify_content method.Data Writing: The Worker collects all findings from the batch and writes them in a single transaction directly to the main ScanFindingSummaries and ScanFindingOccurrences tables.Final Summary: After the database write is successful, the Worker calls complete_task with a simple summary:{
    "status": "COMPLETED",
    "objects_processed": 100,
    "findings_found": 540
}
3.4 Task Type: DELTA_CALCULATEGoal: An internal task to compare a scan's results against the main catalog to find changes.WorkPacket Structure:{
    "header": {
        "task_id": 150,
        "job_id": 15,
        "trace_id": "def-456"
    },
    "payload": {
        "task_type": "DELTA_CALCULATE",
        "staging_table_name": "Staging_DiscoveredObjects_Job_15",
        "config": {}
    }
}
Worker's Internal Workflow:The Worker receives the WorkPacket.This task does not use a Connector. The Worker connects directly to the system's central database.It executes a series of pre-defined, set-based SQL queries to compare the staging_table_name with the main DiscoveredObjects catalog.Data Writing: The SQL queries directly update the main DiscoveredObjects catalog, marking records as NEW, MODIFIED, or DELETED.Final Summary: After the queries complete, the Worker calls complete_task with the results:{
    "status": "COMPLETED",
    "new_count": 1500,
    "modified_count": 350,
    "deleted_count": 80
}
