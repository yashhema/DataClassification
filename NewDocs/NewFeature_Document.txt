-----

## **System Design Document Addendum: Compliance, Entitlement, & Vulnerability Scanning**

### **1. Introduction**

This document outlines the detailed architecture for extending the data classification platform with three new capabilities: **Benchmark Scanning**, **User Entitlement Auditing**, and **CVE Vulnerability Analysis**. This design integrates these features into the existing asynchronous, database-coordinated architecture, prioritizing pragmatic implementation, auditability, and scalability.

-----

### **2. Core Architectural Principles**

The new features are built upon a set of foundational principles to ensure they are robust, manageable, and aligned with enterprise requirements.

  * **The Scan Cycle**: The primary unit for reporting and historical tracking is the **Scan Cycle**. A cycle is a named, time-bound period (e.g., "Q4 2025 Audit"). Within a given cycle, findings represent the latest known state; new scans for the same check will **overwrite (UPSERT)** the previous finding. History and drift are analyzed by comparing results **between** different cycles.

  * **Datasource Profiling**: All advanced scans depend on an accurate profile of the target. A foundational job, **`DATASOURCE_ONBOARDING`**, runs first to discover the target's precise product version, patch level, host OS, and deployment model. This metadata provides the necessary context for all subsequent scans.

  * **Hierarchical Overrides**: A flexible, multi-level override system allows for centralized policy with managed exceptions.

      * **Authority Levels**: Overrides are defined in groups which have an authority level: `Corporate`, `Business`, `Application`, or `Individual`.
      * **Conflict Resolution**: When multiple overrides apply to the same control, the winning rule is determined by a two-tiered logic: **highest authority level wins**. If there's a tie in authority, the rule from the group with the **latest effective date wins**.

  * **Version-Aware Query Management**: Benchmark and entitlement queries are managed centrally. The system uses a "best match" algorithm to select the most specific query for a target's profile, falling back from patch-specific to version-specific to generic queries as needed.

  * [cite\_start]**Connector as Translator**: The **Connector** component [cite: 2013] is responsible for all vendor-specific logic. [cite\_start]It translates a generic request (e.g., "run CIS check 2.1") into the specific SQL dialect needed for that target and normalizes the results into a standard format for the Worker[cite: 1888].

-----

### **3. New Job and Task Types**

To manage the new workflows, the following types will be added to the system's `JobType` and `TaskType` enums.

  * **New `JobType` values**:

      * `DATASOURCE_ONBOARDING`
      * `BENCHMARK`
      * `ENTITLEMENT`
      * `VULNERABILITY`

  * **New `TaskType` values**:

      * `DATASOURCE_PROFILE`
      * `BENCHMARK_EXECUTE`
      * `ENTITLEMENT_EXTRACT`
      * `VULNERABILITY_SCAN`

-----

### **4. Detailed Database Schema**

The following tables will be added to support the new features.

#### **4.1 Datasource Profiling & Metadata**

  * **Table: `datasources` (Modified)**

      * **New Column**: `product_version` (String, Nullable): A human-readable product version.

  * **Table: `datasource_metadata`**

      * **Purpose**: Stores the detailed profile of a data source.
      * **Columns**:
          * `datasource_id` (String, PK, FK): Links to the `datasources` table.
          * `deployment_model` (String): e.g., "SELF\_MANAGED", "CLOUD\_MANAGED", "SAAS".
          * `capabilities` (JSON): A rich object holding discovered properties like `host_os`, `full_version`, `patch_level`, and `edition`.
          * `last_profiled_timestamp` (DateTime)

#### **4.2 Query & Benchmark Management**

  * **Table: `query_sets`**

      * **Purpose**: Groups related queries, such as a specific release version of a benchmark.
      * **Columns**:
          * `id` (PK)
          * `query_set_name` (String): e.g., "CIS\_SQL\_SERVER\_2024".
          * `release_version` (String): e.g., "1.0.0".
          * `target_product` (String): e.g., "sqlserver".
          * `version_match_regex` (String): A regex to match against a discovered product version (e.g., `^15\..*`).
          * `query_type` (String): "BENCHMARK" or "ENTITLEMENT".

  * **Table: `queries`**

      * **Purpose**: Stores the individual, versioned SQL statements for checks.
      * **Columns**:
          * `id` (PK)
          * `query_set_id` (FK to `query_sets`)
          * `check_id` (String): e.g., "2.13".
          * `query_text` (Text): The SQL to be executed.
          * `timeout_seconds` (Integer, Default 300)
          * `version` (Integer), `last_updated_at` (DateTime), `last_updated_by` (String)

#### **4.3 Override & Exception Management**

  * **Table: `override_groups`**, **`datasource_to_override_group_link`**, **`override_rules`**: These tables will be implemented as discussed to manage the hierarchical override framework, with the `override_rules` table containing fields for `override_type`, `exception_status`, `justification`, and `custom_query_text`. The `override_rules` table will also include `is_active` and `expiration_date` fields to manage the override lifecycle.

#### **4.4 Vulnerability (CVE) Catalog**

  * **Tables**: A set of tables (`cve_definitions`, `product_catalog`, `cve_affected_ranges`, `cve_mitigation_options`) will be implemented as per the detailed feedback to store curated CVE data, including version ranges, backports, and mitigation verification queries. Data for these tables will be ingested from a commercial feed like Tenable.

#### **4.5 Finding & Result Storage**

  * **Table: `benchmark_findings`**: Stores the latest benchmark result for a check within a scan cycle. The primary key will be `(datasource_id, check_id, cycle_id)`.

  * **Table: `entitlement_snapshots`**: The parent table for an entitlement scan, enforcing the one-snapshot-per-cycle rule. The primary key will be `snapshot_id`, with a unique constraint on `(datasource_id, cycle_id)`.

  * **Tables: `entitlement_principals`**, **`entitlement_memberships`**, **`entitlement_permissions`**: The three normalized tables that store the details of a snapshot, linked via `snapshot_id`.

  * **Table: `vulnerability_findings`**: Stores the latest vulnerability status for a CVE within a scan cycle. The primary key will be `(datasource_id, cve_id, cycle_id)`.

  * **Table: `sensitive_data_locations`**: A decoupled summary table populated by classification scans, with a primary key on `(datasource_id, table_name)`. It will not have a `cycle_id`.

-----

### **5. Connector Interface Modifications**

A new, specialized interface will be created for connectors that support these advanced scans.

```python
# In core/interfaces/worker_interfaces.py

class IComplianceConnector(ABC):
    @abstractmethod
    async def get_system_profile(self) -> Dict[str, Any]:
        """Returns the target's version, patch level, OS, and deployment model."""
        pass

    @abstractmethod
    async def execute_benchmark_query(self, query_text: str, timeout_sec: int) -> Dict[str, Any]:
        """Executes a single, sandboxed query and returns a standardized result."""
        pass

    @abstractmethod
    async def extract_entitlements(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Extracts the full entitlement snapshot, returning a tuple containing:
        1. The normalized three-part data model (principals, memberships, permissions).
        2. The raw, untransformed data for safekeeping.
        """
        pass
```

-----

### **6. Query Safeguards Framework**

All queries will pass through a two-layer defense system.

1.  **Static Analysis**: Before being saved to the database, queries are analyzed to reject any containing destructive keywords (`UPDATE`, `DELETE`, `DROP`, etc.).
2.  **Runtime Sandboxing**:
      * **Read-Only User**: The connector's database user account **must** have read-only permissions.
      * **Enforced Timeouts**: The connector will enforce the configured execution timeout on every query it runs.

-----

### **7. End-to-End Workflows**

#### **7.1 Workflow: Datasource Profiling**

1.  **Initiation**: An admin triggers a `DATASOURCE_ONBOARDING` job.
2.  [cite\_start]**Tasking**: The Orchestrator [cite: 1665] creates a `DATASOURCE_PROFILE` task.
3.  [cite\_start]**Execution**: A Worker [cite: 1888] gets the task, creates the appropriate Connector, and calls `get_system_profile()`.
4.  **Result**: The Worker receives the profile and calls a database interface method to `UPSERT` the data into the `datasource_metadata` table.

#### **7.2 Workflow: Benchmark Scanning**

1.  **Initiation**: A user starts a `BENCHMARK` job for a specific `cycle_id`.
2.  **Tasking**: The Orchestrator creates a `BENCHMARK_EXECUTE` task.
3.  **Execution (Worker & Connector)**:
    a. The Connector fetches the target's profile from `datasource_metadata`.
    b. It finds the correct `query_set` using the `version_match_regex` and loads the standard queries.
    c. It fetches all applicable `override_rules` for the data source and resolves conflicts using the `Authority > Date` logic.
    d. For each check, it determines whether to use an exception, a custom query, or the standard query.
    e. It executes the plan, calling `execute_benchmark_query()` for each required check.
4.  **Result**: The Worker receives the list of findings and calls the database interface to `UPSERT` them into the `benchmark_findings` table for the specified `cycle_id`.

#### **7.3 Workflow: Entitlement Auditing**

1.  **Initiation**: An `ENTITLEMENT` job is started for a `cycle_id`.
2.  **Tasking**: The Orchestrator creates an `ENTITLEMENT_EXTRACT` task.
3.  **Execution**: The Connector's `extract_entitlements` method is called. It queries the target and performs the complex transformation into the canonical three-table model.
4.  **Result**: The Worker receives the normalized data and calls the database interface to atomically `UPSERT` the entire snapshot for the `cycle_id`, replacing any previous data for that cycle by creating a new snapshot and deleting the old one.

#### **7.4 Workflow: Vulnerability Analysis**

1.  **Prerequisite**: A `DATASOURCE_PROFILE` job has run. A background "CVE Manager" process keeps the CVE tables updated from a commercial feed (e.g., Tenable).
2.  **Tasking**: The Orchestrator creates a `VULNERABILITY_SCAN` task.
3.  **Execution (Worker)**:
    a. The Worker reads the target's profile from `datasource_metadata`.
    b. It queries the internal CVE catalog to find potential vulnerabilities based on product, version, and patch level, using the range-based logic.
    c. For each potential vulnerability, it checks for `cve_mitigation_options` and may execute a verification query via the Connector to determine the final status (`VULNERABLE`, `MITIGATED`, etc.).
4.  **Result**: The Worker `UPSERTs` the final list of vulnerability statuses into the `vulnerability_findings` table for the specified `cycle_id`.








---

## **Addendum A: Implementation Specifications**

This addendum provides critical technical specifications that must be implemented to ensure the architecture functions correctly. These details address error handling, data integrity, and operational workflows that are essential for production deployment.

---

### **A.1 Entitlement Snapshot Atomic Replacement (BLOCKER)**

**Issue:** Section 7.3 describes snapshot replacement but doesn't protect against data loss during partial writes.

**Required Implementation:**

The `entitlement_snapshots` table must include a `snapshot_status` field with values: `LOADING`, `ACTIVE`, `SUPERSEDED`, `FAILED`.

**Atomic Replacement Procedure:**
1. Create new snapshot record with `snapshot_status = 'LOADING'`
2. Write all data to `entitlement_principals`, `entitlement_memberships`, `entitlement_permissions` using new `snapshot_id`
3. If all writes succeed:
   - Update new snapshot: `snapshot_status = 'ACTIVE'`
   - Update previous snapshot: `snapshot_status = 'SUPERSEDED'`
   - Schedule asynchronous cleanup to purge `SUPERSEDED` snapshots after 7-day retention period
4. If any write fails:
   - Update new snapshot: `snapshot_status = 'FAILED'`
   - Leave previous snapshot as `ACTIVE`
   - Log error for operator review

All queries must filter: `WHERE snapshot_status = 'ACTIVE'`

---

### **A.2 Normalized Version Format Specification (BLOCKER)**

**Issue:** Document references version comparison but never defines the canonical format.

**Required Structure in `datasource_metadata.capabilities`:**

```json
{
  "full_version": "15.0.4365.2",
  "normalized_version": "015.000.004365.002",
  "version_parts": {
    "major": 15,
    "minor": 0,
    "build": 4365,
    "revision": 2
  },
  "patch_identifier": "CU18",
  "release_date": "2024-03-15"
}
```

**Normalization Rules:**
- `normalized_version` must be zero-padded to enable lexicographic string comparison
- Format: `[major].[minor].[build].[revision]` with each part padded to uniform width
- SQL Server: 3-digit padding per part (015.000.004365.002)
- PostgreSQL/MySQL: 3-digit major/minor, no build/revision (015.006.000.000)
- Oracle: 3-digit per part for 4-part version (019.021.000.000)

Each connector must implement `normalize_version()` method to produce this format from vendor-specific version strings.

---

### **A.3 Query Selection Fallback Logic (CRITICAL)**

**Issue:** Section 7.2 doesn't specify behavior when no version-matched query set exists.

**Required Addition to Workflow 7.2, Step 3b:**

**Query Selection Hierarchy:**
1. **Exact Match:** `query_set WHERE version_match_regex MATCHES target_version AND target_product = datasource_product`
2. **Generic Fallback:** `query_set WHERE version_match_regex IS NULL AND target_product = datasource_product`
3. **No Match Handling:** If no query set found after fallback, create benchmark findings with `status = 'UNSUPPORTED_VERSION'` for all controls in the requested benchmark

**Requirement:** System administrators must create generic query sets (with `version_match_regex = NULL`) for each supported product to serve as fallback baselines.

---

### **A.4 Scan Cycle State Machine (CRITICAL)**

**Issue:** Cycles are introduced but their lifecycle is never defined.

**Required Schema Addition:**

```sql
scan_cycles:
- cycle_id (PK)
- cycle_name (String, e.g., "Q4_2025_Audit")
- cycle_type (BENCHMARK | VULNERABILITY | ENTITLEMENT)
- status (ACTIVE | CLOSING | CLOSED | ARCHIVED)
- start_date (DateTime)
- end_date (DateTime, nullable)
- created_by (String)
- closed_timestamp (DateTime, nullable)
```

**State Transition Rules:**
- **ACTIVE:** Only one cycle per `cycle_type` can have `status = 'ACTIVE'`. All scans write to the active cycle.
- **CLOSING:** When a new cycle is created, previous ACTIVE cycle transitions to CLOSING. In-flight jobs can complete; new jobs are rejected.
- **CLOSED:** All jobs complete. Cycle becomes immutable. Queries for "current compliance state" use the most recent CLOSED cycle.
- **ARCHIVED:** After retention period (configurable, e.g., 2 years), CLOSED cycles transition to ARCHIVED and data moves to cold storage.

**Database Interface Enforcement:**
- UPSERT operations must validate `cycle_status = 'ACTIVE'`
- Reject writes to CLOSING/CLOSED/ARCHIVED cycles with error
- When creating new cycle, automatically transition previous ACTIVE cycle to CLOSING

---

### **A.5 Entitlement Canonical Format Contract (CRITICAL)**

**Issue:** Section 7.3 references "canonical three-table model" but connectors have no specification for output format.

**Required Connector Interface Contract:**

```python
async def extract_entitlements(self) -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    Returns three lists: (principals, memberships, permissions)
    
    Principal dict structure:
    {
        "principal_name": str,            # e.g., "john.doe", "DOMAIN\\john.doe"
        "principal_type": str,            # USER, ROLE, GROUP, LOGIN
        "authentication_source": str,     # LOCAL, AD, LDAP, IAM, CERTIFICATE, SSO
        "authentication_identity": str,   # SID, ARN, DN, etc. (nullable)
        "can_login": bool,
        "is_enabled": bool,
        "is_builtin": bool,               # System-defined principals
        "principal_attributes": dict      # Vendor-specific JSON
    }
    
    Membership dict structure:
    {
        "member_principal_name": str,
        "container_principal_name": str,
        "membership_type": str,           # DIRECT, NESTED
        "grants_admin": bool,
        "is_inherited": bool
    }
    
    Permission dict structure:
    {
        "principal_name": str,
        "securable_type": str,            # SERVER, DATABASE, SCHEMA, TABLE, COLUMN
        "securable_path": str,            # Hierarchical: "database.schema.table"
        "permission_type": str,           # Normalized: READ, WRITE, EXECUTE, ADMIN, etc.
        "grant_type": str,                # GRANT, DENY
        "is_inherited": bool,             # From role/group membership
        "granted_by": str,                # Grantor principal name (nullable)
        "is_grantable": bool,             # WITH GRANT OPTION
        "vendor_permission_name": str     # Original: "SELECT", "db_owner", etc.
    }
    """
```

All connectors implementing `IComplianceConnector` must adhere to this structure exactly.

---

### **A.6 CVE Mitigation Verification Error Handling (CRITICAL)**

**Issue:** Section 7.4 mentions executing verification queries but provides no failure model.

**Required Error Handling in Workflow 7.4, Step 3c:**

When executing mitigation verification queries:

**Error Scenarios and Responses:**
- **Query Timeout:** Set `status = 'VERIFICATION_TIMEOUT'`, log timeout duration, continue with remaining CVEs
- **Access Denied:** Set `status = 'VERIFICATION_FAILED'`, log permission error, continue scan
- **Unexpected Schema:** Set `status = 'VERIFICATION_ERROR'`, log schema mismatch, continue scan
- **Query Returns NULL:** Treat as mitigation NOT present (conservative approach), set `status = 'VULNERABLE'`
- **Success:** Set `status = 'VULNERABLE'` or `status = 'MITIGATED'` based on query result

**Schema Requirement:** `vulnerability_findings.status` enum must include: `VULNERABLE`, `MITIGATED`, `PATCHED`, `NOT_APPLICABLE`, `VERIFICATION_TIMEOUT`, `VERIFICATION_FAILED`, `VERIFICATION_ERROR`.

**Critical Rule:** Never fail the entire vulnerability scan due to a single verification query failure. Log errors and continue processing remaining CVEs.

---

### **A.7 Override Expiration Behavior (IMPORTANT)**

**Issue:** Document mentions `expiration_date` but doesn't define behavior when overrides expire.

**Required Logic:**

Before applying any override rule, system must check:
```
IF override_rule.expiration_date IS NOT NULL 
   AND override_rule.expiration_date < current_timestamp 
   AND override_rule.is_active = TRUE
THEN
   - Ignore this override
   - Use standard query or next-priority override
   - Log warning: "Override {id} expired on {date}, reverting to standard query"
```

**Override Lifecycle:**
- **Expired Overrides:** Remain in database with `is_active = TRUE` for audit trail
- **Disabled Overrides:** Set `is_active = FALSE` to permanently deactivate (preserves history)
- **UI Display:** Admin interface must show status as "EXPIRED" for rules past `expiration_date`

**For EXCEPTION Overrides:** Expiration should trigger notification 30 days prior, requiring explicit renewal with updated justification.

---

### **A.8 Job Execution Traceability (IMPORTANT)**

**Issue:** Findings reference `job_execution_id` but workflow doesn't show its origin.

**Required Addition to Workflow 7.2, Step 2:**

The Orchestrator creates `BENCHMARK_EXECUTE` task with `WorkPacket.payload` containing:
- `job_id`: Unique identifier for this job instance (generated by Orchestrator)
- `task_id`: Unique identifier for this specific task
- `cycle_id`: Target reporting cycle
- `query_set_id`: Which benchmark is being executed
- `datasource_id`: Target datasource
- `active_override_group_ids`: List of override group IDs applicable to target (resolved by Orchestrator)

**Worker Responsibility:** Store `job_id` from WorkPacket as `job_execution_id` in all finding records.

**Traceability Chain:** Finding record → `job_execution_id` → Job execution metadata → Complete audit of which queries/overrides were used

---

### **A.9 Sensitive Data Locations Population Workflow (MISSING)**

**Issue:** Table `sensitive_data_locations` is defined but its population mechanism is never explained.

**Required New Section 7.5:**

#### **7.5 Workflow: Sensitive Data Location Enrichment**

This background process runs after classification scans to maintain the summary table used by entitlement analysis.

**Trigger:** Classification job completes with status `COMPLETED`

**Process:**
1. Background enrichment service queries `scan_findings` table
2. For completed classification job:
   ```sql
   SELECT 
     datasource_id,
     table_name,
     ARRAY_AGG(DISTINCT classification_type) as classifications
   FROM scan_findings
   WHERE job_execution_id = :completed_job_id
     AND classification_type IS NOT NULL
   GROUP BY datasource_id, table_name
   ```
3. UPSERT results into `sensitive_data_locations`
4. Update `last_updated_timestamp` for affected datasource

**Result:** Table reflects latest known sensitive data locations. Entitlement scans reference this to focus analysis on high-risk objects.

**Note:** `sensitive_data_locations` is decoupled from scan cycles—it always represents "most recent classification findings" regardless of which cycle the classification ran in.

---

### **A.10 Permission Type Normalization Mapping (MISSING)**

**Issue:** Document shows normalized permission types but doesn't provide complete translation rules for connectors.

**Required Normalization Rules (Partial List):**

**Microsoft SQL Server:**
- `SELECT`, `VIEW DEFINITION` → `READ`
- `INSERT`, `UPDATE`, `DELETE` → `WRITE`
- `EXECUTE` → `EXECUTE`
- `CONTROL`, `db_owner`, `sysadmin` → `ADMIN`
- `ALTER` → `ALTER`
- `CREATE [object_type]` → `CREATE`

**PostgreSQL:**
- `SELECT` → `READ`
- `INSERT`, `UPDATE`, `DELETE` → `WRITE`
- `EXECUTE` → `EXECUTE`
- `ALL PRIVILEGES`, `OWNER` → `ADMIN`
- `CREATE` → `CREATE`
- `USAGE` (on schema) → `READ`
- `USAGE` (on sequence) → `WRITE`

**Oracle Database:**
- `SELECT` → `READ`
- `INSERT`, `UPDATE`, `DELETE` → `WRITE`
- `EXECUTE` → `EXECUTE`
- `DBA`, `SYSDBA`, `ALL` → `ADMIN`
- `ALTER` → `ALTER`
- `CREATE SESSION` → (no permission record; handled via `can_login` in principals)

**MongoDB:**
- `read` → `READ`
- `readWrite` → `WRITE`
- `dbAdmin`, `userAdmin`, `root` → `ADMIN`

**Snowflake:**
- `USAGE` (on database/schema) → `READ`
- `SELECT` → `READ`
- `INSERT`, `UPDATE`, `DELETE` → `WRITE`
- `OWNERSHIP`, `ACCOUNTADMIN` → `ADMIN`

**Connector Requirement:** Each connector must implement this mapping logic. Store both:
- `permission_type`: Normalized value for cross-platform queries
- `vendor_permission_name`: Original vendor-specific value for audit/troubleshooting

### **A.11 Copy DataSourceMetadata  profile to job - for vulnerability, cisbenchmark **
As part of the job configuration - copy the current DataSourceMetadata  profile of the db , so we always know what profile we ran the job against , it will be required for audit purposes
---

**End of Addendum A**